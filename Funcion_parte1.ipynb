{"cells":[{"cell_type":"code","source":["dbutils.library.installPyPI(\"geopandas\")\ndbutils.library.installPyPI(\"shapely\")\nimport sys\nsys.version"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"eaa482f5-f1c5-4d31-975a-8a320dffc49b"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">Out[3]: &#39;3.7.5 (default, Nov  7 2019, 10:50:52) \\n[GCC 8.3.0]&#39;</div>","removedWidgets":[],"addedWidgets":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Out[3]: &#39;3.7.5 (default, Nov  7 2019, 10:50:52) \\n[GCC 8.3.0]&#39;</div>"]}}],"execution_count":0},{"cell_type":"code","source":["import pyspark\nfrom pyspark.sql.types import StructType, StructField, DoubleType, StringType, DateType, IntegerType, FloatType\nfrom pyspark.sql.functions import year, month, dayofmonth\nfrom pyspark.sql.utils import AnalysisException\nimport pyspark.sql.functions as F\nimport pandas as pd\nfrom shapely.geometry import Point, Polygon, shape\nfrom shapely import wkb, wkt\nimport geopandas as gpd\nimport numpy as np"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"b5b4d864-ff69-4bff-bcbb-b45e3c3c5a57"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"code","source":["import pyspark\nfrom pyspark.sql.types import StructType, StructField, DoubleType, StringType, DateType, IntegerType, FloatType\nfrom pyspark.sql.functions import year, month, dayofmonth\nfrom pyspark.sql.utils import AnalysisException\nimport pyspark.sql.functions as F\nimport pandas as pd\nfrom shapely.geometry import Point, Polygon, shape\nfrom shapely import wkb, wkt\nimport geopandas as gpd\nimport numpy as np\n\n\n      \ndef transform_coma(algo):\n  lista_final = []\n  for h in algo:\n    st_h = str(h)\n    new_ok = st_h.replace(\",\", \"\").replace(\"[\",\"\").replace(\"]\",\"\")\n    lista_final.append(new_ok)\n  return lista_final\n\n\ndef wkt_loads(x):\n    try:\n        return wkt.loads(x)\n    except Exception:\n        return None\n\ndef primera_parte(data_file,json_file):  \n  df_schema_1 = StructType() \\\n            .add(field= \"VendorID\", data_type=StringType(), nullable= True) \\\n            .add(field=\"tpep_pickup_datetime\", data_type=DateType(), nullable= True) \\\n            .add(field=\"tpep_dropoff_datetime\", data_type=DateType(), nullable= True) \\\n            .add(field=\"passenger_count\", data_type=IntegerType(), nullable= True) \\\n            .add(field=\"trip_distance\", data_type=DoubleType(), nullable= True) \\\n            .add(field=\"pickup_longitude\", data_type=FloatType(), nullable= True) \\\n            .add(field=\"pickup_latitude\", data_type=FloatType(), nullable= True) \\\n            .add(field=\"RatecodeID\", data_type=IntegerType(), nullable= True) \\\n            .add(field=\"store_and_fwd_flag\", data_type=StringType(), nullable= True) \\\n            .add(field=\"dropoff_longitude\", data_type=FloatType(), nullable= True) \\\n            .add(field=\"dropoff_latitude\", data_type=FloatType(), nullable= True) \\\n            .add(field=\"payment_type\", data_type=IntegerType(), nullable= True) \\\n            .add(field=\"fare_amount\", data_type=DoubleType(), nullable= True) \\\n            .add(field=\"extra\", data_type=DoubleType(), nullable= True) \\\n            .add(field=\"mta_tax\", data_type=DoubleType(), nullable= True) \\\n            .add(field=\"tip_amount\", data_type=DoubleType(), nullable= True) \\\n            .add(field=\"tolls_amount\", data_type=DoubleType(), nullable= True) \\\n            .add(field=\"improvement_surcharge\", data_type=DoubleType(), nullable= True) \\\n            .add(field=\"total_amount\", data_type=DoubleType(), nullable= True)\n  \n  df_coordinates_schema = StructType()\\\n                      .add(field = \"zone_id\", data_type=StringType(), nullable= True)\\\n                      .add(field = \"zone_name\", data_type=StringType(), nullable= True)\\\n                      .add(field = \"geom\", data_type=StringType(), nullable= True)\n  \n  df_with_schema = spark.read.format(\"csv\") \\\n      .option(\"header\", True) \\\n      .schema(df_schema_1) \\\n      .load(data_file)\n  \n  df_with_schema_col = df_with_schema.withColumn(\"year\", year(df_with_schema.tpep_pickup_datetime)) \\\n            .withColumn(\"month\", month(df_with_schema.tpep_pickup_datetime)) \\\n            .withColumn(\"day\", dayofmonth(df_with_schema.tpep_pickup_datetime)) \n  \n  #PARTE B\n  data_json = spark.read.json(json_file)\n  location_id = data_json.select(\"features.properties.locationid\")\n  segment_location_id = location_id.select(\"locationid\").collect()\n  lista_id = segment_location_id[0][0] #Lista de ID_Zonas extraidas del json\n  lista_id_2 = [int(segment_location_id[0][0][i]) for i in range(len(lista_id))] \n  name_location = data_json.select(\"features.properties.zone\")\n  segment_name_location = name_location.select(\"zone\").collect()\n  lista_name = segment_name_location[0][0] #Lista de nombres extraidas del json \n\n  geometry_zone = data_json.select(\"features.geometry.coordinates\")\n  segment_geometry_zone = geometry_zone.select(\"coordinates\").collect()\n  \n  geo_without_coma =[transform_coma(segment_geometry_zone[0][0][i][0][0]) for i in range(len(segment_geometry_zone[0][0]))]\n  #geo_with_coma =[segment_geometry_zone[0][0][i][0][0] for i in range(len(segment_geometry_zone[0][0]))]\n  lista_geometry_zone = [\"Multipolygon \" + str([[geo_without_coma[i]]]).replace(\"[\",\"(\").replace(\"]\",\")\").replace(\"'\",\"\") for i in range(len(geo_without_coma))]\n  \n  id_name_coordinate_tuple = [(lista_id[i],lista_name[i],lista_geometry_zone[i]) for i in range(len(lista_name))] \n  pruebas = [wkt.loads(lista_geometry_zone[i]) for i in range(len(lista_geometry_zone))]\n  \n  #pruebas_2 = [{\"type\":\"Multipolygon \", \"coordinates\":[geo_with_coma[i]]} for i in range(len(geo_without_coma))]\n  #print(pruebas_2)\n  \n  data_id_zone_coordinates = {\n  'zone_id':lista_id_2,\n  'zone_name': lista_name,\n  'geom': pruebas\n  }\n\n  #mk_sp_df = spark.createDataFrame(data=data_id_zone_coordinates, schema=df_coordinates_schema)\n  df_id_zone_coordinates = pd.DataFrame(data_id_zone_coordinates, columns=['zone_id', 'zone_name', 'geom']) #Dataframe para utilizar con geopandas\n  #df_id_zone_coordinates = df_id_zone_coordinates.assign(geom=pruebas)\n  #df_id_zone_coordinates['geom']=df_id_zone_coordinates['geom'].apply(wkt.loads)\n  gdf  = gpd.GeoDataFrame(df_id_zone_coordinates, geometry='geom')\n  sc.broadcast(gdf)\n  def name_zone(value_1, lista=id_name_coordinate_tuple):\n    for i,j,k in lista:\n      if(i==value_1):\n        return j\n    return 'Locacion NO registrada'\n\n  def id_zone(value_1, lista=id_name_coordinate_tuple):\n    for i,j,k in lista:\n      if(j==value_1):\n        return i\n    return 'Locacion NO registrada'\n  \n  id_zone_UDF = F.udf(lambda x:id_zone(x), IntegerType())\n  name_zone_UDF = F.udf(lambda x:name_zone(x))\n  \n  def find_borough(latitude, longitude): \n    mgdf = gdf.apply(lambda x: x['zone_name'] if x['geom'].intersects(Point(longitude,latitude)) else None, axis=1)\n    idx = mgdf.first_valid_index()\n    first_valid_value = mgdf.loc[idx] if idx is not None else None\n    return first_valid_value\n  \n  find_borough_udf = F.udf(find_borough, StringType())\n  \n  df_raw_PU = df_with_schema_col.withColumn(\"pickup_zone\", find_borough_udf(df_with_schema_col.pickup_latitude,df_with_schema_col.pickup_longitude))\\\n    .withColumn(\"dropoff_zone\", find_borough_udf(df_with_schema_col.dropoff_latitude,df_with_schema_col.dropoff_longitude))\n  df_final = df_raw_PU.withColumn(\"PULocationID\", id_zone_UDF(df_raw_PU.pickup_zone))\\\n            .withColumn(\"DOLocationID\", id_zone_UDF(df_raw_PU.dropoff_zone))\n  \n  return df_final\n  \n\n#\"dbfs:/FileStore/tables/yellow_tripdata_2016_03.csv\"\n#display(primera_parte(\"dbfs:/FileStore/tables/yellow_tripdata_2016_03.csv\",\"/FileStore/tables/nyu_2451_36743_geojson.json\"))"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"d75e6711-ed90-4cb4-8234-80240030f554"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"code","source":["#TAREA 2, PARTE 1 PUNTO A\n#esctructura para datos que estan desde 2017 para delante\ndf_schema = StructType() \\\n            .add(field= \"VendorID\", data_type=StringType(), nullable= True) \\\n            .add(field=\"tpep_pickup_datetime\", data_type=DateType(), nullable= True) \\\n            .add(field=\"tpep_dropoff_datetime\", data_type=DateType(), nullable= True) \\\n            .add(field=\"passenger_count\", data_type=IntegerType(), nullable= True) \\\n            .add(field=\"trip_distance\", data_type=DoubleType(), nullable= True) \\\n            .add(field=\"RatecodeID\", data_type=IntegerType(), nullable= True) \\\n            .add(field=\"store_and_fwd_flag\", data_type=StringType(), nullable= True) \\\n            .add(field=\"PULocationID\", data_type=IntegerType(), nullable= True) \\\n            .add(field=\"DOLocationID\", data_type=IntegerType(), nullable= True) \\\n            .add(field=\"payment_type\", data_type=IntegerType(), nullable= True) \\\n            .add(field=\"fare_amount\", data_type=DoubleType(), nullable= True) \\\n            .add(field=\"extra\", data_type=DoubleType(), nullable= True) \\\n            .add(field=\"mta_tax\", data_type=DoubleType(), nullable= True) \\\n            .add(field=\"tip_amount\", data_type=DoubleType(), nullable= True) \\\n            .add(field=\"tolls_amount\", data_type=DoubleType(), nullable= True) \\\n            .add(field=\"improvement_surcharge\", data_type=DoubleType(), nullable= True) \\\n            .add(field=\"total_amount\", data_type=DoubleType(), nullable= True) \\\n            .add(field=\"congestion_surcharge\", data_type=DoubleType(), nullable= True)\n\n#eschema para datos de 2013 hasta 2017\ndf_schema_1 = StructType() \\\n            .add(field= \"VendorID\", data_type=StringType(), nullable= True) \\\n            .add(field=\"tpep_pickup_datetime\", data_type=DateType(), nullable= True) \\\n            .add(field=\"tpep_dropoff_datetime\", data_type=DateType(), nullable= True) \\\n            .add(field=\"passenger_count\", data_type=IntegerType(), nullable= True) \\\n            .add(field=\"trip_distance\", data_type=DoubleType(), nullable= True) \\\n            .add(field=\"pickup_longitude\", data_type=FloatType(), nullable= True) \\\n            .add(field=\"pickup_latitude\", data_type=FloatType(), nullable= True) \\\n            .add(field=\"RatecodeID\", data_type=IntegerType(), nullable= True) \\\n            .add(field=\"store_and_fwd_flag\", data_type=StringType(), nullable= True) \\\n            .add(field=\"dropoff_longitude\", data_type=FloatType(), nullable= True) \\\n            .add(field=\"dropoff_latitude\", data_type=FloatType(), nullable= True) \\\n            .add(field=\"payment_type\", data_type=IntegerType(), nullable= True) \\\n            .add(field=\"fare_amount\", data_type=DoubleType(), nullable= True) \\\n            .add(field=\"extra\", data_type=DoubleType(), nullable= True) \\\n            .add(field=\"mta_tax\", data_type=DoubleType(), nullable= True) \\\n            .add(field=\"tip_amount\", data_type=DoubleType(), nullable= True) \\\n            .add(field=\"tolls_amount\", data_type=DoubleType(), nullable= True) \\\n            .add(field=\"improvement_surcharge\", data_type=DoubleType(), nullable= True) \\\n            .add(field=\"total_amount\", data_type=DoubleType(), nullable= True)\n\ndef has_column(df, col):\n    try:\n        df[col]\n        return True\n    except AnalysisException:\n        return False\n            \nfile_location = \"dbfs:/FileStore/tables/yellow_tripdata_2016_03.csv\"\n\ndf_prueba = spark.read.format(\"csv\") \\\n      .option(\"header\", True) \\\n      .load(file_location)\n\nif (has_column(df_prueba, \"PULocationID\")):\n  \n  print(\"primero\")\n  df_with_schema = spark.read.format(\"csv\") \\\n      .option(\"header\", True) \\\n      .schema(df_schema) \\\n      .load(file_location)\n  df_with_schema_col = df_with_schema.withColumn(\"year\", year(df_with_schema.tpep_pickup_datetime)) \\\n              .withColumn(\"month\", month(df_with_schema.tpep_pickup_datetime)) \\\n              .withColumn(\"day\", dayofmonth(df_with_schema.tpep_pickup_datetime)) \n\t\t\t  \n  display(df_with_schema_col)\nelse:\n  print(\"segundo\")\n  df_with_schema = spark.read.format(\"csv\") \\\n      .option(\"header\", True) \\\n      .schema(df_schema_1) \\\n      .load(file_location)\n  df_with_schema_col = df_with_schema.withColumn(\"year\", year(df_with_schema.tpep_pickup_datetime)) \\\n              .withColumn(\"month\", month(df_with_schema.tpep_pickup_datetime)) \\\n              .withColumn(\"day\", dayofmonth(df_with_schema.tpep_pickup_datetime)) \n\t\t\t  \n  #display(df_with_schema_col)\n  \n#display(df_with_schema_col.groupBy('PULocationID').count())\n\n#pu_location_id = df_with_schema_col.select(\"PULocationID\").collect()\n\n\n\n\n"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"84486dd4-eaea-4cfb-83a1-4643fd7ef14b"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">segundo\n</div>","removedWidgets":[],"addedWidgets":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">segundo\n</div>"]}}],"execution_count":0},{"cell_type":"code","source":["#PARTE 1 PUNTO B\nfile_location = \"/FileStore/tables/nyu_2451_36743_geojson.json\"\ndata = spark.read.json(file_location)\nlocation_id = data.select(\"features.properties.locationid\")\nsegment_location_id = location_id.select(\"locationid\").collect()\nlista_id = segment_location_id[0][0] #Lista de ID_Zonas extraidas del json\nname_location = data.select(\"features.properties.zone\")\nsegment_name_location = name_location.select(\"zone\").collect()\nlista_name = segment_name_location[0][0] #Lista de nombres extraidas del json \n\ngeometry_zone = data.select(\"features.geometry.coordinates\")\nsegment_geometry_zone = geometry_zone.select(\"coordinates\").collect()\n\n#geom = shape(pruebitas)\n#print(geom.wkt)\n\ndef transform_coma(algo):\n  lista_final = []\n  for h in algo:\n    st_h = str(h)\n    new_ok = st_h.replace(\",\", \"\").replace(\"[\",\"\").replace(\"]\",\"\")\n    lista_final.append(new_ok)\n  return lista_final\n\n#Arregral el jason para usar geopandas\n\n#quitar las comas de dentro\ngeo_without_coma =[transform_coma(segment_geometry_zone[0][0][i][0][0]) for i in range(len(segment_geometry_zone[0][0]))]\n\n#reemplazar brackets por parentesis\nlista_geometry_zone = [\"Multipolygon \" + str([[geo_without_coma[i]]]).replace(\"[\",\"(\").replace(\"]\",\")\").replace(\"'\",\"\") for i in range(len(geo_without_coma))]\n\nid_name_coordinate_tuple = [(lista_id[i],lista_name[i],lista_geometry_zone[i]) for i in range(len(lista_name))] \n\n#aplicandole shapely a cada uno de los elementos de la lista\npruebas = [wkt.loads(lista_geometry_zone[i]) for i in range(len(lista_geometry_zone))]\n\ndata_id_zone_coordinates = {\n  'id': lista_id,\n  'zone_name': lista_name,\n  'geom': pruebas\n}\ndf_id_zone_coordinates = pd.DataFrame(data_id_zone_coordinates, columns=['id', 'zone_name', 'geom']) #Dataframe para utilizar con geopandas\n\ndef name_zone(value_1, lista = id_name_coordinate_tuple):\n  for i,j,k in lista:\n    if(i==value_1):\n      return j\n  return 'Locacion NO registrada'\n  \ndef id_zone(value_1, lista = id_name_coordinate_tuple):\n  for i,j,k in lista:\n    if(j==value_1):\n      return i\n  return 'Locacion NO registrada'\n  \nid_zone_UDF = F.udf(lambda x:id_zone(x), IntegerType())\nname_zone_UDF = F.udf(lambda x:name_zone(x))\n  \n  \nif (has_column(df_with_schema_col, \"PULocationID\")):\n  \n  df_final = df_with_schema_col.withColumn('PULocationZone',name_zone_UDF(df_with_schema_col.PULocationID))\\\n                  .withColumn('DOLocationZone',name_zone_UDF(df_with_schema_col.DOLocationID))\n  #display(df_final)\n\nelse:\n  \n  #df_id_zone_coordinates['geom']=df_id_zone_coordinates['geom'].apply(wkt.loads)\n  gdf  = gpd.GeoDataFrame(df_id_zone_coordinates, geometry='geom')\n    \n  def find_borough(latitude, longitude): \n    mgdf = gdf.apply(lambda x: x['zone_name'] if x['geom'].intersects(Point(longitude,latitude)) else None, axis=1)\n    idx = mgdf.first_valid_index()\n    first_valid_value = mgdf.loc[idx] if idx is not None else None\n    return first_valid_value\n    \n  find_borough_udf = F.udf(lambda x,y: find_borough(x,y))\n  df_raw_PU = df_with_schema_col.withColumn(\"pickup_zone\", find_borough_udf(df_with_schema_col.pickup_latitude,df_with_schema_col.pickup_longitude))\\\n    .withColumn(\"dropoff_zone\", find_borough_udf(df_with_schema_col.dropoff_latitude,df_with_schema_col.dropoff_longitude))\n    \n    \n  df_final = df_raw_PU.withColumn(\"PULocationID\", id_zone_UDF(df_raw_PU.pickup_zone))\\\n            .withColumn(\"DOLocationID\", id_zone_UDF(df_raw_PU.dropoff_zone))\n  display(df_final)\n"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"3849c56d-18cc-44c3-97f9-dca6f615e44e"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}},{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"type":"ipynbError","data":"","errorSummary":"Cancelled","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"]}}],"execution_count":0},{"cell_type":"code","source":["#PARTE 2 PUNTO C \npu_location_list = [row.PULocationID for row in df_final.select('PULocationID').collect()]\n#pu_tuple = [(pu_location_list_id[i],pu_location_list_name) for i in range(len(pu_location_list_name))]\ndo_location_list = [row.DOLocationID for row in df_with_schema_col.select('DOLocationID').collect()]\nmatriz_OD_zeros = np.zeros([len(lista_id)+1,len(lista_name)+1\n                           ])\n\nfor i in range(1, len(pu_location_list)):\n  if (pu_location_list[i]<=263 and do_location_list[i]<=263):\n    matriz_OD_zeros[pu_location_list[i]][do_location_list[i]] +=1 \n    \n    \ndata_correlacion = pd.DataFrame(matriz_OD_zeros)\n#display(data_correlacion)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"622b0dfd-c629-42c1-a3f5-459a6313dc7f"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}},{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"type":"ipynbError","data":"","errorSummary":"Cancelled","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"]}}],"execution_count":0},{"cell_type":"code","source":["#PARTE 2 Punto D\nfrecuent_places = df_final.groupBy('DOLocationID').count().orderBy('count',ascending=False)\nfrecuent_places_names = frecuent_places.withColumn('NameLocation',name_zone_UDF(frecuent_places.DOLocationID)).take(10)\n#display(frecuent_places_names)\n# agrupar por años tambien"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"f0354bc9-e5cf-47cf-896a-e3fe142e455a"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}},{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"type":"ipynbError","data":"","errorSummary":"Cancelled","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"]}}],"execution_count":0},{"cell_type":"code","source":["#PARTE 2 PUNTO E\ntotal_trip_distance = df_final.groupBy('year').agg({'trip_distance':'sum'})\n#display(total_trip_distance)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"6df7927d-9438-4e63-a04d-6ea73b581d12"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"code","source":["#PARTE 2 PUNTO G\n# Cuanto se recauda anualmente con cada medio de pago\npayement_type=df_final.groupBy('payment_type').agg({'total_amount':'sum'}).orderBy('sum(total_amount)',ascending=False)\n#display(payement_type)\n\n"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"5165e306-a145-4998-bd7f-361cdc2eddaf"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0}],"metadata":{"application/vnd.databricks.v1+notebook":{"notebookName":"tarea2","dashboards":[],"language":"python","widgets":{},"notebookOrigID":645837060739463}},"nbformat":4,"nbformat_minor":0}
