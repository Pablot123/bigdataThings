{"cells":[{"cell_type":"code","source":["dbutils.library.installPyPI(\"geopandas\")\ndbutils.library.installPyPI(\"shapely\")"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"eaa482f5-f1c5-4d31-975a-8a320dffc49b"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">PyPI package geopandas has been installed already. The previously installed package is `geopandas`. To resolve this issue, detach and re-attach the notebook to create a new environment or rename the package.\nPyPI package shapely has been installed already. The previously installed package is `shapely`. To resolve this issue, detach and re-attach the notebook to create a new environment or rename the package.\nOut[176]: False</div>","removedWidgets":[],"addedWidgets":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">PyPI package geopandas has been installed already. The previously installed package is `geopandas`. To resolve this issue, detach and re-attach the notebook to create a new environment or rename the package.\nPyPI package shapely has been installed already. The previously installed package is `shapely`. To resolve this issue, detach and re-attach the notebook to create a new environment or rename the package.\nOut[176]: False</div>"]}}],"execution_count":0},{"cell_type":"code","source":["import pyspark\nfrom pyspark.sql.types import StructType, StructField, DoubleType, StringType, DateType, IntegerType, FloatType\nfrom pyspark.sql.functions import year, month, dayofmonth\nfrom pyspark.sql.utils import AnalysisException\nimport pyspark.sql.functions as F\nimport pandas as pd\nfrom shapely.geometry import Point, Polygon, shape\nfrom shapely import wkb, wkt\nimport geopandas as gpd\nimport numpy as np"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"b5b4d864-ff69-4bff-bcbb-b45e3c3c5a57"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"code","source":["#TAREA 2, PARTE 1 PUNTO A\n#esctructura para datos que estan desde 2017 para delante\ndf_schema = StructType() \\\n            .add(field= \"VendorID\", data_type=StringType(), nullable= True) \\\n            .add(field=\"tpep_pickup_datetime\", data_type=DateType(), nullable= True) \\\n            .add(field=\"tpep_dropoff_datetime\", data_type=DateType(), nullable= True) \\\n            .add(field=\"passenger_count\", data_type=IntegerType(), nullable= True) \\\n            .add(field=\"trip_distance\", data_type=DoubleType(), nullable= True) \\\n            .add(field=\"RatecodeID\", data_type=IntegerType(), nullable= True) \\\n            .add(field=\"store_and_fwd_flag\", data_type=StringType(), nullable= True) \\\n            .add(field=\"PULocationID\", data_type=IntegerType(), nullable= True) \\\n            .add(field=\"DOLocationID\", data_type=IntegerType(), nullable= True) \\\n            .add(field=\"payment_type\", data_type=IntegerType(), nullable= True) \\\n            .add(field=\"fare_amount\", data_type=DoubleType(), nullable= True) \\\n            .add(field=\"extra\", data_type=DoubleType(), nullable= True) \\\n            .add(field=\"mta_tax\", data_type=DoubleType(), nullable= True) \\\n            .add(field=\"tip_amount\", data_type=DoubleType(), nullable= True) \\\n            .add(field=\"tolls_amount\", data_type=DoubleType(), nullable= True) \\\n            .add(field=\"improvement_surcharge\", data_type=DoubleType(), nullable= True) \\\n            .add(field=\"total_amount\", data_type=DoubleType(), nullable= True) \\\n            .add(field=\"congestion_surcharge\", data_type=DoubleType(), nullable= True)\n\n#eschema para datos de 2013 hasta 2017\ndf_schema_1 = StructType() \\\n            .add(field= \"VendorID\", data_type=StringType(), nullable= True) \\\n            .add(field=\"tpep_pickup_datetime\", data_type=DateType(), nullable= True) \\\n            .add(field=\"tpep_dropoff_datetime\", data_type=DateType(), nullable= True) \\\n            .add(field=\"passenger_count\", data_type=IntegerType(), nullable= True) \\\n            .add(field=\"trip_distance\", data_type=DoubleType(), nullable= True) \\\n            .add(field=\"pickup_longitude\", data_type=FloatType(), nullable= True) \\\n            .add(field=\"pickup_latitude\", data_type=FloatType(), nullable= True) \\\n            .add(field=\"RatecodeID\", data_type=IntegerType(), nullable= True) \\\n            .add(field=\"store_and_fwd_flag\", data_type=StringType(), nullable= True) \\\n            .add(field=\"dropoff_longitude\", data_type=FloatType(), nullable= True) \\\n            .add(field=\"dropoff_latitude\", data_type=FloatType(), nullable= True) \\\n            .add(field=\"payment_type\", data_type=IntegerType(), nullable= True) \\\n            .add(field=\"fare_amount\", data_type=DoubleType(), nullable= True) \\\n            .add(field=\"extra\", data_type=DoubleType(), nullable= True) \\\n            .add(field=\"mta_tax\", data_type=DoubleType(), nullable= True) \\\n            .add(field=\"tip_amount\", data_type=DoubleType(), nullable= True) \\\n            .add(field=\"tolls_amount\", data_type=DoubleType(), nullable= True) \\\n            .add(field=\"improvement_surcharge\", data_type=DoubleType(), nullable= True) \\\n            .add(field=\"total_amount\", data_type=DoubleType(), nullable= True)\n\ndef has_column(df, col):\n    try:\n        df[col]\n        return True\n    except AnalysisException:\n        return False\n            \nfile_location = \"dbfs:/FileStore/tables/yellow_tripdata_2016_03.csv\"\n\ndf_prueba = spark.read.format(\"csv\") \\\n      .option(\"header\", True) \\\n      .load(file_location)\n\nif (has_column(df_prueba, \"PULocationID\")):\n  print(\"primero\")\n  df_with_schema = spark.read.format(\"csv\") \\\n      .option(\"header\", True) \\\n      .schema(df_schema) \\\n      .load(file_location)\n  df_with_schema_col = df_with_schema.withColumn(\"year\", year(df_with_schema.tpep_pickup_datetime)) \\\n              .withColumn(\"month\", month(df_with_schema.tpep_pickup_datetime)) \\\n              .withColumn(\"day\", dayofmonth(df_with_schema.tpep_pickup_datetime)) \n\t\t\t  \n  #display(df_with_schema_col)\nelse:\n  print(\"segundo\")\n  df_with_schema = spark.read.format(\"csv\") \\\n      .option(\"header\", True) \\\n      .schema(df_schema_1) \\\n      .load(file_location)\n  df_with_schema_col = df_with_schema.withColumn(\"year\", year(df_with_schema.tpep_pickup_datetime)) \\\n              .withColumn(\"month\", month(df_with_schema.tpep_pickup_datetime)) \\\n              .withColumn(\"day\", dayofmonth(df_with_schema.tpep_pickup_datetime)) \n\t\t\t  \n  #display(df_with_schema_col)\n  \n#display(df_with_schema_col.groupBy('PULocationID').count())\n\n#pu_location_id = df_with_schema_col.select(\"PULocationID\").collect()\n\n\n\n\n"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"84486dd4-eaea-4cfb-83a1-4643fd7ef14b"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">segundo\n</div>","removedWidgets":[],"addedWidgets":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">segundo\n</div>"]}}],"execution_count":0},{"cell_type":"code","source":["#PARTE 1 PUNTO B\nfile_location = \"/FileStore/tables/nyu_2451_36743_geojson.json\"\ndata = spark.read.json(file_location)\nlocation_id = data.select(\"features.properties.locationid\")\nsegment_location_id = location_id.select(\"locationid\").collect()\nlista_id = segment_location_id[0][0] #Lista de ID_Zonas extraidas del json\n\nname_location = data.select(\"features.properties.zone\")\nsegment_name_location = name_location.select(\"zone\").collect()\nlista_name = segment_name_location[0][0] #Lista de nombres extraidas del json \n\ngeometry_zone = data.select(\"features.geometry.coordinates\")\nsegment_geometry_zone = geometry_zone.select(\"coordinates\").collect()\n\n\ndef transform_coma(algo):\n  lista_final = []\n  for h in algo:\n    st_h = str(h)\n    new_ok = st_h.replace(\",\", \"\").replace(\"[\",\"\").replace(\"]\",\"\")\n    lista_final.append(new_ok)\n  return lista_final\n\n#Arregral el jason para usar geopandas\n#quitar las comas de dentro\ngeo_without_coma =[transform_coma(segment_geometry_zone[0][0][i][0][0]) for i in range(len(segment_geometry_zone[0][0]))]\n\n#reemplazar brackets por parentesis\nlista_geometry_zone = [\"Multipolygon \" + str([[geo_without_coma[i]]]).replace(\"[\",\"(\").replace(\"]\",\")\").replace(\"'\",\"\") for i in range(len(lista_geometry_zone)) ]\n\nid_name_coordinate_tuple = [(lista_id[i],lista_name[i],lista_geometry_zone[i]) for i in range(len(lista_name))] \ndata_id_zone_coordinates = {\n  'id': lista_id,\n  'zone_name': lista_name,\n  'geom': lista_geometry_zone\n}\ndf_id_zone_coordinates = pd.DataFrame(data_id_zone_coordinates, columns=['id', 'zone_name', 'geom']) #Dataframe para utilizar con geopandas\n\ndef name_zone(value_1, lista = id_name_coordinate_tuple):\n  for i,j,k in lista:\n    if(i==value_1):\n      return j\n  return 'Locacion NO registrada'\n\n  \n  \ndef id_zone(value_1, lista = id_name_coordinate_tuple):\n  for i,j,k in lista:\n    if(j==value_1):\n      return i\n  return 'Locacion NO registrada'\n  \nid_zone_UDF = F.udf(lambda x:id_zone(x), IntegerType())\nname_zone_UDF = F.udf(lambda x:name_zone(x))\n  \n  \nif (has_column(df_with_schema_col, \"PULocationID\")):\n  df_final = df_with_schema_col.withColumn('PULocationZone',name_zone_UDF(df_with_schema_col.PULocationID))\\\n                  .withColumn('DOLocationZone',name_zone_UDF(df_with_schema_col.DOLocationID))\n  #display(df_final)\n\nelse:\n  \n  df_id_zone_coordinates['geom']=df_id_zone_coordinates['geom'].apply(wkt.loads)\n  gdf  = gpd.GeoDataFrame(df_id_zone_coordinates, geometry='geom')\n    \n  def find_borough(latitude, longitude): \n    mgdf = gdf.apply(lambda x: x['zone_name'] if x['geom'].intersects(Point(longitude,latitude)) else None, axis=1)\n    idx = mgdf.first_valid_index()\n    first_valid_value = mgdf.loc[idx] if idx is not None else None\n    return first_valid_value\n    \n  find_borough_udf = F.udf(lambda x,y: find_borough(x,y))\n  df_raw_PU = df_with_schema_col.withColumn(\"pickup_zone\", find_borough_udf(df_with_schema_col.pickup_latitude,df_with_schema_col.pickup_longitude))\\\n    .withColumn(\"dropoff_zone\", find_borough_udf(df_with_schema_col.dropoff_latitude,df_with_schema_col.dropoff_longitude))\n    \n    \n  df_final = df_raw_PU.withColumn(\"PULocationID\", id_zone_UDF(df_raw_PU.pickup_zone))\\\n            .withColumn(\"DOLocationID\", id_zone_UDF(df_raw_PU.dropoff_zone))\n  #display(df_final)\n"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"3849c56d-18cc-44c3-97f9-dca6f615e44e"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"code","source":["\npu_location_list = [row.PULocationID for row in df_final.select('PULocationID').collect()]\n#pu_tuple = [(pu_location_list_id[i],pu_location_list_name) for i in range(len(pu_location_list_name))]\ndo_location_list = [row.DOLocationID for row in df_with_schema_col.select('DOLocationID').collect()]\nmatriz_OD_zeros = np.zeros([len(lista_id),len(lista_name)])\n\nfor i in range(1, len(pu_location_list)):\n  if i>263:\n    pass\n  else:\n    matriz_OD_zeros[pu_location_list[i]][do_location_list[i]] +=1 \n    \ndata = pd.DataFrame(matriz_OD_zeros)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"622b0dfd-c629-42c1-a3f5-459a6313dc7f"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}},{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"type":"ipynbError","data":"","errorSummary":"Cancelled","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"]}}],"execution_count":0},{"cell_type":"code","source":["#Punto 2D\nfrecuent_places = df_final.groupBy('DOLocationID').count().orderBy('count',ascending=False)\nfrecuent_places_names = frecuent_places.withColumn('NameLocation',name_zone_UDF(frecuent_places.DOLocationID))\n\n"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"f0354bc9-e5cf-47cf-896a-e3fe142e455a"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0}],"metadata":{"application/vnd.databricks.v1+notebook":{"notebookName":"tarea2","dashboards":[],"language":"python","widgets":{},"notebookOrigID":645837060739463}},"nbformat":4,"nbformat_minor":0}
